{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import dotenv_values\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "config = dotenv_values(\".env\")\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    openai_api_key=config[\"OPENAI_API_KEY\"],\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[\n",
    "        StreamingStdOutCallbackHandler(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# https://python.langchain.com/docs/versions/migrating_memory/\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=chat,\n",
    "    max_token_limit=20,\n",
    "    return_messages=True,\n",
    "    memory_key=\"history\",\n",
    ")\n",
    "\n",
    "cache_path = \"./.cache\"\n",
    "embedding_path = f\"{cache_path}/challenge-04\"\n",
    "Path(embedding_path).mkdir(parents=True, exist_ok=True)\n",
    "embedding_dir = LocalFileStore(embedding_path)\n",
    "\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "loader = TextLoader(\"./challenge-04.txt\")\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    openai_api_key=config[\"OPENAI_API_KEY\"],\n",
    ")\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
    "    embeddings,\n",
    "    embedding_dir,\n",
    ")\n",
    "vectorstore = FAISS.from_documents(docs, cached_embeddings)\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer questions using only the following context.\\n{context}\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def load_memory(_):\n",
    "    return memory.load_memory_variables({})[\"history\"]\n",
    "\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": retriever,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "        \"history\": load_memory,\n",
    "    }\n",
    "    | prompt\n",
    "    | chat\n",
    ")\n",
    "\n",
    "\n",
    "def invoke_chain(question):\n",
    "    result = chain.invoke(question)\n",
    "    memory.save_context(\n",
    "        {\"input\": question},\n",
    "        {\"output\": result.content},\n",
    "    )\n",
    "    # print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aaronson은 유죄가 아닙니다. 그는 죄를 지은 적이 없으며, 그에 대한 유죄 판결은 잘못된 것입니다.The human asks if Aaronson is guilty. The AI responds that Aaronson is not guilty, stating that he has never committed a crime and that the guilty verdict against him is incorrect."
     ]
    }
   ],
   "source": [
    "invoke_chain(\"Aaronson 은 유죄인가요? (Is Aaronson guilty?)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "그는 테이블에 다음과 같은 메시지를 썼습니다: \"FREEDOM IS SLAVERY\" (자유는 노예다) 그리고 그 아래에 \"TWO AND TWO MAKE FIVE\" (둘과 둘은 다섯이다)라고 썼습니다.The human asks if Aaronson is guilty. The AI responds that Aaronson is not guilty, stating that he has never committed a crime and that the guilty verdict against him is incorrect. The human then inquires about a message Aaronson wrote on the table, to which the AI reveals that he wrote \"FREEDOM IS SLAVERY\" and \"TWO AND TWO MAKE FIVE.\""
     ]
    }
   ],
   "source": [
    "invoke_chain(\n",
    "    \"그가 테이블에 어떤 메시지를 썼나요? (What message did he write in the table?)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Julia는 Winston의 사랑하는 사람으로, 그와 함께 자유를 추구하고 반당파적인 관계를 맺었던 인물입니다. 그녀는 Winston과 함께 당의 억압에 맞서 싸우려 했지만, 결국 그들의 관계는 당의 감시와 압박에 의해 파괴됩니다.The human asks if Aaronson is guilty. The AI responds that Aaronson is not guilty, stating that he has never committed a crime and that the guilty verdict against him is incorrect. The human then inquires about a message Aaronson wrote on the table, to which the AI reveals that he wrote \"FREEDOM IS SLAVERY\" and \"TWO AND TWO MAKE FIVE.\" The human then asks about Julia, and the AI explains that Julia is Winston's beloved, who sought freedom alongside him and had a rebellious relationship, but their bond was ultimately destroyed by the Party's surveillance and pressure."
     ]
    }
   ],
   "source": [
    "invoke_chain(\"Julia 는 누구인가요? (Who is Julia?)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
