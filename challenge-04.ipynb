{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import dotenv_values\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "config = dotenv_values(\".env\")\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    openai_api_key=config[\"OPENAI_API_KEY\"],\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[\n",
    "        StreamingStdOutCallbackHandler(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# https://python.langchain.com/docs/versions/migrating_memory/\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=chat,\n",
    "    max_token_limit=20,\n",
    "    return_messages=True,\n",
    "    memory_key=\"history\",\n",
    ")\n",
    "\n",
    "cache_path = \"./.cache\"\n",
    "embedding_path = f\"{cache_path}/challenge-04\"\n",
    "Path(embedding_path).mkdir(parents=True, exist_ok=True)\n",
    "embedding_cache_dir = LocalFileStore(embedding_path)\n",
    "\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "loader = TextLoader(\"./challenge-04.txt\")\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=config[\"OPENAI_API_KEY\"])\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
    "    embeddings, embedding_cache_dir\n",
    ")\n",
    "vectorstore = FAISS.from_documents(docs, cached_embeddings)\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            Answer the question using ONLY the following context. \n",
    "            If you don't know the answer just say you don't know. \n",
    "            DON't make anything up.\n",
    "\n",
    "            Context: {context}\n",
    "            \"\"\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def load_memory(_):\n",
    "    return memory.load_memory_variables({})[\"history\"]\n",
    "\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": retriever,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "        \"history\": load_memory,\n",
    "    }\n",
    "    | prompt\n",
    "    | chat\n",
    ")\n",
    "\n",
    "\n",
    "def invoke_chain(question):\n",
    "    result = chain.invoke(question)\n",
    "    memory.save_context(\n",
    "        {\"input\": question},\n",
    "        {\"output\": result.content},\n",
    "    )\n",
    "    # print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aaronson은 유죄가 아닙니다.The human asks if Aaronson is guilty."
     ]
    }
   ],
   "source": [
    "invoke_chain(\"Aaronson 은 유죄인가요? (Is Aaronson guilty?)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "그는 테이블에 \"FREEDOM IS SLAVERY\", \"TWO AND TWO MAKE FIVE\", \"GOD IS POWER\"라는 메시지를 썼습니다.The human asks if Aaronson is guilty. The AI responds that Aaronson is not guilty. The human then inquires about the message Aaronson wrote on the table, to which the AI reveals that he wrote \"FREEDOM IS SLAVERY,\" \"TWO AND TWO MAKE FIVE,\" and \"GOD IS POWER.\""
     ]
    }
   ],
   "source": [
    "invoke_chain(\n",
    "    \"그가 테이블에 어떤 메시지를 썼나요? (What message did he write in the table?)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Julia는 Winston의 사랑하는 사람이며, 그들과 함께 반당파적인 활동을 하던 인물입니다. 그녀는 Winston과의 관계를 통해 개인적인 자유와 사랑을 추구하지만, 결국 당의 압박과 감시 아래에서 고통받게 됩니다.The human asks if Aaronson is guilty. The AI responds that Aaronson is not guilty. The human then inquires about the message Aaronson wrote on the table, to which the AI reveals that he wrote \"FREEDOM IS SLAVERY,\" \"TWO AND TWO MAKE FIVE,\" and \"GOD IS POWER.\" The human then asks about Julia, and the AI explains that Julia is Winston's lover who engaged in anti-party activities with him, seeking personal freedom and love but ultimately suffering under the Party's oppression and surveillance."
     ]
    }
   ],
   "source": [
    "invoke_chain(\"Julia 는 누구인가요? (Who is Julia?)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
